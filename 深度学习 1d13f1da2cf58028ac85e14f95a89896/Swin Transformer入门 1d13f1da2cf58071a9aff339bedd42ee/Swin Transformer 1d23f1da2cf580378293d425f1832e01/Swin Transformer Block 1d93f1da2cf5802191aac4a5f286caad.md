# Swin Transformer Block

<aside>
ğŸ‘“

@whiszk 
04/18/2025 

</aside>

---

## ä¸€ã€Blockä¸»ä½“ç»“æ„

blockéƒ½æ˜¯**æˆå¯¹å‡ºç°**ï¼Œä¸€ä¸ªstageä¸­åªä¼šæœ‰å¶æ•°ä¸ªblockï¼Œå› ä¸ºW-MSAå’ŒSWâ€”MSAéœ€è¦äº¤æ›¿åº”ç”¨

![ä¸€ç»„Swin Transformer Block
       ï¼ˆvisioç»˜åˆ¶ï¼‰](Swin%20Transformer%20Block%201d93f1da2cf5802191aac4a5f286caad/%E7%BB%98%E5%9B%BE2.jpg)

ä¸€ç»„Swin Transformer Block
       ï¼ˆvisioç»˜åˆ¶ï¼‰

ä»£ç åŒ…å«ä»¥ä¸‹å­æ¨¡å—ï¼š

- W-MSA**æˆ–**SW-MSAï¼Œé€šè¿‡`WindowAttention`è°ƒç”¨
- `Window_partition`ï¼Œç®€å•å‡½æ•°ï¼Œ`(B, H, W, C)`->`(num_windows*B, window_size, window_size, C)`
- LayerNorm
- mlp
- æ®‹å·®è¿æ¥

```python
class SwinTransformerBlock(nn.Module):
    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                 fused_window_process=False):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        
        # è¾“å…¥å›¾ç‰‡å°ºå¯¸å°äºè®¾ç½®çš„msaçª—å£å¤§å°æ—¶ï¼Œç›´æ¥ä½¿ç”¨å›¾ç‰‡å°ºå¯¸ä½œä¸ºmsaçª—å£å¤§å°ï¼Œä¸”ä¸ç§»ä½
        if min(self.input_resolution) <= self.window_size:
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
					
				 # ç§»ä½æ­¥é•¿å¤§äº0ï¼Œè¡¨ç¤ºè¿™ä¸ªblockè¦ä½¿ç”¨sw-msaï¼Œæ­¤å¤„è®¡ç®—ç”Ÿæˆä¸€ä¸ªæ³¨æ„åŠ›æ©ç 
**A**       if self.shift_size > 0:
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            
            # ä¸ºæ¯ä¸ªåŒºåŸŸåˆ†é…å”¯ä¸€ ID
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

						 # æ ¹æ®idï¼Œç”Ÿæˆæ³¨æ„åŠ›maskçŸ©é˜µï¼Œä¸forwardä¸­çš„å¾ªç¯ç§»ä½æ­é…ä½¿ç”¨
 **B**          mask_windows = window_partition(img_mask, self.window_size)  # çª—å£æ•°é‡, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size) # å±•å¹³
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer("attn_mask", attn_mask)
        self.fused_window_process = fused_window_process # è®¡ç®—ä¼˜åŒ–å¼€å…³ï¼Œéœ€è¦cudaæ”¯æŒ

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"

        shortcut = x # ä¿å­˜åŸå§‹è¾“å…¥ï¼Œç”¨äºåç»­æ®‹å·®è¿æ¥
        x = self.norm1(x) # å…ˆå¯¹é€šé“ç»´åº¦Cè¿›è¡Œlayernorm
        x = x.view(B, H, W, C) # å°†patchåºåˆ—æ¢å¤ä¸ºç©ºé—´æ’åˆ—çš„äºŒç»´ç½‘æ ¼ï¼Œä¾¿äºåç»­çš„çª—å£åˆ’åˆ†å’Œç§»ä½æ“ä½œ

        # å¾ªç¯ç§»ä½ï¼Œå‘å·¦å‘ä¸Šç§»åŠ¨
        if self.shift_size > 0:
            if not self.fused_window_process:
                shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
                x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
            else:
                x_windows = WindowProcess.apply(x, B, H, W, C, -self.shift_size, self.window_size)
        else:
            shifted_x = x
            x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
				
				 # å±•å¹³ï¼ŒnW*B, window_size*window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)

        # ä¼ å…¥W-MSA/SW-MSAæ¨¡å—
        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C

				 # æ¢å¤å±•å¹³å‰çš„çª—å£å½¢çŠ¶
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)

        # æ¢å¤ä½ç§»å‰çš„patchä½ç½®
        if self.shift_size > 0:
            if not self.fused_window_process:
                shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C
                x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
            else:
                x = WindowProcessReverse.apply(attn_windows, B, H, W, C, self.shift_size, self.window_size)
        else:
            shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C
            x = shifted_x
        x = x.view(B, H * W, C)
        x = shortcut + self.drop_path(x) # ç¬¬ä¸€æ¬¡æ®‹å·®è¿æ¥

        # ä¸€è¡Œå®Œæˆä¸‰ä¸ªæ“ä½œï¼šç¬¬äºŒæ¬¡layernormï¼Œmlpï¼Œæ®‹å·®è¿æ¥
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, " \
               f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}"

    def flops(self):
        flops = 0
        H, W = self.input_resolution
        # norm1
        flops += self.dim * H * W
        # W-MSA/SW-MSA
        nW = H * W / self.window_size / self.window_size
        flops += nW * self.attn.flops(self.window_size * self.window_size)
        # mlp
        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
        # norm2
        flops += self.dim * H * W
        return flops
```

---

### 1.å¾ªç¯ç§»ä½çš„é€»è¾‘é“¾

çª—å£ç§»åŠ¨æ˜¯ä»¥patchç§»åŠ¨å®ç°çš„(ç›¸å¯¹è¿åŠ¨)ï¼Œä»¥`Wï¼ŒH=6` `windowsize=3` `shiftsize = 1`ä¸¾ä¾‹ï¼š

åŸä»£ç ä¸­çš„**Aéƒ¨åˆ†**ï¼š

- åœ¨`init`å‡½æ•°ä¸­ï¼Œå…ˆé€šè¿‡sliceä¸foræ¥**åˆ†é…åŒºåŸŸid**ï¼ŒåŸæ¥åœ¨åŒä¸€ä¸ªwindowä¸­ï¼Œæˆ–è€…åœ¨ä¸¤ä¸ªä¸åŒwindowä½†ç´§æŒ¨ï¼ˆéµå¾ªè¾¹ç•Œï¼Œä¸è¿›è¡Œç©ºé—´è·¨è¶Šï¼‰ç€çš„patchï¼Œä¼šè¢«åˆ†é…åˆ°ç›¸åŒçš„id
- å¾—åˆ°çš„`img_mask`Â æ˜¯ä¸€ä¸ªÂ `[1, 6, 6, 1]`Â çš„å¼ é‡ï¼Œå†…å®¹ä¸ºï¼š
    
    ```markdown
    [
      [[[0]], [[0]], [[0]], [[1]], [[1]], [[2]]],  # ç¬¬0è¡Œ
      [[[0]], [[0]], [[0]], [[1]], [[1]], [[2]]],  # ç¬¬1è¡Œ
      [[[0]], [[0]], [[0]], [[1]], [[1]], [[2]]],  # ç¬¬2è¡Œ
      [[[3]], [[3]], [[3]], [[4]], [[4]], [[5]]],  # ç¬¬3è¡Œ
      [[[3]], [[3]], [[3]], [[4]], [[4]], [[5]]],  # ç¬¬4è¡Œ
      [[[6]], [[6]], [[6]], [[7]], [[7]], [[8]]]   # ç¬¬5è¡Œ
    ]
    ```
    
    å¯è§†åŒ–å±•ç¤ºï¼šå·¦è¾¹çš„éƒ¨åˆ†ä»£è¡¨W-MSAçš„çª—å£åˆ’åˆ†ä¸æ³¨æ„åŠ›è®¡ç®—ï¼Œå¯¹äº**å³è¾¹çš„éƒ¨åˆ†**ï¼Œé¦–å…ˆæ˜ç¡®åªæœ‰åœ¨åŒä¸€ä¸ªwindowä¸­æ‰å¯ä»¥äº’ç›¸attentionï¼Œè¿›ä¸€æ­¥çš„ï¼Œåœ¨å•ä¸ªwindowä¸­ï¼Œåªæœ‰**åŒºåŸŸidç›¸åŒçš„patch**ï¼ˆçº¢æ¡†åœˆèµ·æ¥çš„éƒ¨åˆ†ï¼Œä¸ä¸Šæ–‡åŒºåŸŸä»£ç å¯¹åº”ï¼‰ï¼Œæ‰å¯ä»¥äº’ç›¸attentionã€‚ç›¸å½“äºåœ¨å¤§windowä¸­å¼•å…¥äº†å°windowã€‚
    
    ![                    å¾ªç¯ç§»ä½ä¸maskæ³¨æ„åŠ›è®¡ç®—ç¤ºæ„å›¾ï¼ˆpptç»˜åˆ¶ï¼‰](Swin%20Transformer%20Block%201d93f1da2cf5802191aac4a5f286caad/%E6%BC%94%E7%A4%BA%E6%96%87%E7%A8%BF1.jpg)
    
                        å¾ªç¯ç§»ä½ä¸maskæ³¨æ„åŠ›è®¡ç®—ç¤ºæ„å›¾ï¼ˆpptç»˜åˆ¶ï¼‰
    

å¯¹è¿™æ ·çš„åˆ’åˆ†æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·ç†è§£ï¼š

- åœ¨ä¸€æ¬¡wmsaè¿‡åï¼Œæˆ‘ä»¬å¸Œæœ›ä¸åŒwindowä¸­ç›¸é‚»çš„patchå…³æ³¨ä¸€ä¸‹å½¼æ­¤ï¼Œ**è®©äººä¸ºçš„çª—å£åˆ’åˆ†ä¸é‚£ä¹ˆç”Ÿç¡¬**ï¼Œä½†æ˜¯åŸæ¥é‚£äº›ä¸æŒ¨ç€çš„ï¼Œå°±æ²¡å¿…è¦äº¤æµäº†ï¼Œè¿™å°±å®Œæˆäº†ä¸€è½®swmsaï¼›
- å†è¿›å…¥ä¸‹ä¸€è½®wmsaæ—¶ï¼Œå› ä¸ºwindowä¸­çš„éƒ¨åˆ†patchå·²ç»äº’ç›¸äº¤æµè¿‡äº†ï¼Œä»–ä»¬ä¸ä¼šè¿‡äºé™Œç”Ÿï¼Œå¯¹æ•´ä¸ªwindowåšattentionæ—¶å°±ä¸ä¼šæ˜¾å¾—çªå…€ï¼Œæ‰€ä»¥**å®é™…ä¸Šçš„æ»‘åŠ¨çª—å£ï¼Œæœ‰ä¸¤æ­¥èµ°çš„è¿‡ç¨‹**ã€‚
- éœ€è¦æ˜ç¡®çš„æ˜¯ï¼šæ— è®ºè¿›è¡Œå¤šå°‘æ¬¡ç§»ä½ï¼Œæ‰€æœ‰patchçš„**ç»å¯¹ç‰©ç†åæ ‡**å§‹ç»ˆä¸å˜ï¼ˆé™é‡‡æ ·ä¸ç®—ï¼‰ï¼Œåæ ‡ä½ç½®åœ¨é¢„å¤„ç†é˜¶æ®µå°±ç¡®å®šäº†ï¼Œç§»ä½åªæ˜¯æš‚æ—¶çš„ï¼Œå‰ä¸€è½®wmsaä¸­ï¼Œé‚£äº›ä¸ç›¸é‚»ä¸”ä¸åœ¨ä¸€ä¸ªwindowä¸­çš„patchï¼Œä¸ä¼šåœ¨åäºŒè½®wmsaä¸­å…³æ³¨å½¼æ­¤ï¼Œä¿¡æ¯çš„ä¼ é€’æ°¸è¿œæ˜¯é—´æ¥çš„ï¼Œå› ä¸ºç§»ä½åªæ˜¯ä¼ é€’ä¿¡æ¯çš„æ‰‹æ®µï¼Œæ˜¯å¤šæ¬¡ç§»ä½è®©ç›¸éš”å¾ˆè¿œçš„patché—´æ¥äº¤æµï¼Œè€Œä¸æ˜¯ç–¯ç‹‚æ‰“ä¹±patchä½ç½®æ¥èµ·åˆ°äº¤æµçš„ç»“æœã€‚
- æ‰“ä¸ªæ¯”æ–¹ï¼Œè‡ªä¹ æ•™å®¤ä¸­ä½äºä¸¤ä¸ªè§’è½çš„äººï¼Œé€šè¿‡ä¸­é—´äººå‡†ç¡®ä¼ é€’ä¿¡æ¯ï¼Œè€Œ**ä¸æ˜¯ç¦»å¼€åº§ä½äº¤æµæˆ–è€…ç›´æ¥å¤§å–Šã€‚**

---

åŸä»£ç ä¸­çš„**Béƒ¨åˆ†**ï¼š

- ä¹‹å‰çš„åŒºåŸŸåˆ’åˆ†è¿˜éœ€è¦å¤„ç†æ‰èƒ½ç”¨äºæ³¨æ„åŠ›è®¡ç®—ï¼Œä¸‹æ–‡ä¸ºæ¯ä¸ªpatchæä¾›äº†é•¿åº¦ä¸º9çš„æ³¨æ„åŠ›åºåˆ—ï¼ŒåŒ…å«äº†æ‰€å±çª—å£çš„æ‰€æœ‰patchï¼Œå¦‚æœåŒºåŸŸidç›¸åŒï¼Œæ‰ä¼šæ­£å¸¸è®¡ç®—æ³¨æ„åŠ›
    
    ```python
    # partitionä¹‹åçš„ç»´åº¦ï¼š çª—å£æ•°é‡, window_size, window_size, 1
    mask_windows = window_partition(img_mask, self.window_size)  
    
    mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
    '''
    å±•å¹³ä¹‹åï¼š
    tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [1., 1., 2., 1., 1., 2., 1., 1., 2.],
            [3., 3., 3., 3., 3., 3., 6., 6., 6.],
            [4., 4., 5., 4., 4., 5., 7., 7., 8.]])
    '''
    
    # unsqueezeå¯åœ¨æŒ‡å®šä½ç½®æ’å…¥ä¸€ä¸ªæ–°ç»´åº¦ï¼Œä¸€èˆ¬æ­é…å¹¿æ’­æœºåˆ¶ä½¿ç”¨
    #	æ‰€æœ‰ä½ç½®å¯¹è¿›è¡Œç›¸å‡ï¼Œå¾—åˆ°window_size*2ä¸ºé«˜å®½çš„çŸ©é˜µ
    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
    
    # ç›¸å‡ç»“æœä¸ä¸º0çš„ï¼Œç½®-100ï¼Œè¿™æ ·åœ¨softmaxæ—¶ä¼šè¢«ç›´æ¥å¿½ç•¥ï¼Œåªæœ‰æ¥è‡ªåŒåŒºåŸŸçš„patchå…è®¸è®¡ç®—æ³¨æ„åŠ›
    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
    ```
    

---

### 2.ä½ç§»ä¸æ¢å¤ä½ç§»

`x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))`

æ³¨æ„åˆ°åœ¨ä¼ å…¥attentionæ¨¡å—å¾—åˆ°ç»“æœåï¼Œæ¢å¤äº†ä½ç§»å‰çš„patchä½ç½®ï¼Œæ›´æ˜ç¡®äº†[è¿™ä¸€ç‚¹](Swin%20Transformer%20Block%201d93f1da2cf5802191aac4a5f286caad.md)

---

## **äºŒã€W-MSAå’ŒSW-MSA**

- åŒæ—¶æ”¯æŒ**W-MSA**å’Œ**SW-MSA**çš„æ¨¡å—
    - W-MSAé˜¶æ®µï¼šçª—å£å†…è‡ªç”±äº¤æµï¼ˆå±€éƒ¨ä¿¡æ¯æ•´åˆï¼‰
    - SW-MSAé˜¶æ®µï¼šè®©**ç‰©ç†ç›¸é‚»ä½†è¢«çª—å£åˆ†å‰²**çš„patchå»ºç«‹è¿æ¥ï¼ˆéå¸¸é‡è¦ï¼‰
- å¯¹è¾“å…¥å›¾åƒåšä¸€ä¸ªå›ºå®šå¤§å°çª—å£ï¼ˆå¦‚ `7Ã—7`ï¼‰åˆ’åˆ†ï¼Œæ¯ä¸ªçª—å£å†…åš Self-Attentionï¼Œä¸Transformerçš„åŒºåˆ«æ˜¯ï¼šæ³¨æ„åŠ›ä¸å†å…¨å±€è®¡ç®—ï¼Œåªåœ¨çª—å£å†…éƒ¨è¿›è¡Œï¼Œè®¡ç®—é‡å¤§å¹…é™ä½ï¼Œä¸”swintçš„self attentionä¼šå®æ—¶æ„ŸçŸ¥ä½ç½®ï¼Œé€šè¿‡å¼•å…¥**ç›¸å¯¹ä½ç½®åç½®è¡¨**å®ç°ï¼š
    - å¦‚æœä¸¤ä¸ªä½ç½®å¯¹çš„ç›¸å¯¹åæ ‡å·®ï¼ˆÎ”h, Î”wï¼‰ç›¸åŒï¼Œé‚£ä¹ˆå®ƒä»¬çš„ç›¸å¯¹ä½ç½®åç½®ä¹Ÿä¼šç›¸åŒï¼Œå³å¯¹æ‰€æœ‰å¤´çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œéƒ½é‡‡ç”¨åŒä¸€å¥—ä½ç½®åç§»é‡
    - æ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½æœ‰ä¸€å¥—ç‹¬ç«‹çš„ç›¸å¯¹ä½ç½®åç½®å‚æ•°ï¼Œå‚æ•°æ€»ç»´åº¦ä¸º **`[ä½ç½®å¯¹æ€»æ•°, æ³¨æ„åŠ›å¤´æ•°]`**ï¼Œç”¨äºè¡¥å……ç©ºé—´ç»“æ„ä¿¡æ¯ï¼Œä½¿æ³¨æ„åŠ›æœºåˆ¶å…·å¤‡ä½ç½®ä¿¡æ¯çš„å»ºæ¨¡èƒ½åŠ›
    

```python
class WindowAttention(nn.Module):
    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.dim = dim # patchçš„é€šé“æ•°ï¼Œå³C
        self.window_size = window_size  # attention windowçš„é«˜ä¸å®½
        self.num_heads = num_heads # æ³¨æ„åŠ›å¤´çš„æ•°é‡
        head_dim = dim // nudm_heads
        self.scale = qk_scale or head_dim ** -0.5 # åè€…æ˜¯é»˜è®¤çš„\frac{1}{\sqrt{d_k}}

        # å®šä¹‰ç›¸å¯¹ä½ç½®åç½®å‚æ•°ï¼Œç»´åº¦æ˜¯[2*Windowh-1 * 2*Windoww-1, num_heads]
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))

        # æ„é€ ç›¸å¯¹ä½ç½®ä½ç½®ç´¢å¼•è¡¨ï¼ˆä¸å¯å­¦ä¹ ï¼‰ï¼šç”¨äºç´¢å¼•åç½®è¡¨
        coords_h = torch.arange(self.window_size[0]) # ç”Ÿæˆ0->size-1çš„ä¸€ç»´å¼ é‡ï¼Œä¸ºåˆ›å»ºè¡¨æ ¼æä¾›åæ ‡
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # å¹¿æ’­æœºåˆ¶ï¼Œ 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - 1 
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer("relative_position_index", relative_position_index) # ä¿å­˜ä¸ºé™æ€å‚æ•°ï¼Œä¸å‚ä¸è®­ç»ƒ

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        # è¿™ä¸ªB_æ˜¯æ‰€æœ‰å›¾ç‰‡ä¸­çš„æ€»çª—å£æ•°ï¼Œæ˜¯çª—å£åˆ’åˆ†ä¹‹åçš„ç»“æœï¼ŒB_=num_windowsï¼ˆä¸€å¼ å›¾ç‰‡çš„çª—å£æ•°ï¼‰ * B
        B_, N, C = x.shape
        
        # ä»çº¿æ€§å±‚åˆå§‹åŒ–å¤šå¤´qkvçŸ©é˜µ
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2] 

        q = q * self.scale # ç›¸å½“äº\sqrt{d_k}
        attn = (q @ k.transpose(-2, -1)) # QK^Tï¼Œattnå½¢çŠ¶ä¸º[B_, num_heads, Wh*Ww, Wh*Ww]
        
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        attn = attn + relative_position_bias.unsqueeze(0) # åŠ ä¸Šä½ç½®åç½®
                
        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn) # å¯¹attnæœ€åä¸€ä¸ªç»´åº¦ï¼ˆå³æ¯ä¸ªqueryå¯¹åº”çš„æ‰€æœ‰keyï¼‰åšSoftmax
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x # è¾“å‡ºç»´åº¦ä¸º[B_, N, C]

    def extra_repr(self) -> str:
        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'

    def flops(self, N): # Nä¸ºpatchæ•°
        flops = 0
        # qkv = self.qkv(x)
        flops += N * self.dim * 3 * self.dim
        # attn = (q @ k.transpose(-2, -1))
        flops += self.num_heads * N * (self.dim // self.num_heads) * N
        #  x = (attn @ v)
        flops += self.num_heads * N * N * (self.dim // self.num_heads)
        # x = self.proj(x)
        flops += N * self.dim * self.dim
        return flops
```

---

### 1.ç›¸å¯¹ä½ç½®ç´¢å¼•è¡¨çš„æ„é€ 

- æ„é€ **ç›¸å¯¹ä½ç½®ä½ç½®ç´¢å¼•è¡¨**çš„è¿‡ç¨‹ä¸­ï¼Œ`meshgrid`ä¼šè¿”å›ä¸¤ä¸ªçŸ©é˜µï¼Œ**`stack`**å°†è¿™ä¸¤ä¸ªçŸ©é˜µå †å ï¼Œå¾—åˆ°**`coords`ï¼Œ**ä¸ºäº†ä¾¿äºè®¡ç®—ç›¸å¯¹ä½ç½®ï¼Œè¿˜è¦å†ç»è¿‡å±•å¹³ï¼Œä¹‹åçš„ç›¸å¯¹ä½ç½®è®¡ç®—æ¯”è¾ƒç¹çï¼Œå…¶ä¸­åº”ç”¨äº†å¹¿æ’­æœºåˆ¶
- è¿™é‡Œç›´æ¥ç»™å‡º`relative_position_index`çš„ç»“æœï¼Œæ¯”å¦‚è¾“å…¥åæ ‡æ˜¯ä¸¤ä¸ª`[0,1,2]`ï¼Œåˆ™ï¼š
    
    ```python
     meshgridç»“æœï¼š                      stackä¹‹åï¼š        
    [[0, 0, 0],     [[0, 1, 2],        [
     [1, 1, 1],      [0, 1, 2],         [[0, 0, 0], [1, 1, 1], [2, 2, 2]],  # é«˜åº¦åæ ‡
     [2, 2, 2]]      [0, 1, 2]]         [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   # å®½åº¦åæ ‡
    																			 [
    
    flattenå±•å¹³ä¹‹åï¼š
    [
      [0,0,0,1,1,1,2,2,2],  # æ‰€æœ‰ä½ç½®çš„è¡Œåæ ‡
      [0,1,2,0,1,2,0,1,2]    # æ‰€æœ‰ä½ç½®çš„åˆ—åæ ‡ï¼Œä¸Šä¸‹pairè¡¨ç¤ºä¸€ä¸ªç‚¹
    ]			
    
    â€¦â€¦â€¦â€¦â€¦
    
    relative_position_indexï¼š
    
    i\j	  (0,0)	(0,1)	(0,2)	(1,0)	(1,1)	(1,2)	(2,0)	(2,1)	(2,2)
    (0,0)	12	   11	   10	    7	    6	    5 	    2	    1	    0
    (0,1)	13	   12	   11	    8	    7	    6	    3	    2	    1
    (0,2)	14	   13	   12	    9	    8	    7	    4	    3	    2
    (1,0)	17	   16	   15	    12	    11	    10	    7	    6	    5
    (1,1)	18	   17	   16	    13	    12	    11	    8	    7	    6
    (1,2)	19	   18	   17	    14	    13	    12	    9	    8	    7
    (2,0)	22	   21	   20	    17	    16	    15	    12	    11	    10
    (2,1)	23	   22	   21	    18	    17	    16	    13	    12	    11
    (2,2)	24	   23	   22	    19	    18	    17	    14	    13	    12			 
    ```
    

---

### 2.ç›¸å¯¹ä½ç½®åç§»å‚æ•°

å…³äº`self.relative_position_bias_table`çš„**å‚æ•°æ•°é‡**å¦‚ä½•ç¡®å®šï¼š

- å¦‚æœçª—å£å¤§å°æ˜¯`7Ã—7`ï¼Œä¹Ÿå°±æ˜¯çª—å£ä¸­æœ‰49ä¸ªtokenï¼Œé‚£ä¹ˆå¯¹ä»»æ„tokenå¯¹ï¼Œå®ƒä»¬çš„æ¨ªå‘ç›¸å¯¹ä½ç§»èŒƒå›´æ˜¯`-6 ~ +6`ï¼Œå…± `13` ç§ï¼Œè¿™ä¸ª13å°±æ˜¯2*windowsize-1çš„ç»“æœï¼Œçºµå‘ä¹Ÿæ˜¯ä¸€æ ·ï¼Œæ‰€ä»¥åœ¨äºŒç»´ç©ºé—´ä¸­ï¼Œä¸€å…±æœ‰`13Ã—13`ç§ç›¸å¯¹ä½ç½®
- å› ä¸ºæˆ‘ä»¬åªå…³å¿ƒ**ç›¸å¯¹ä½ç½®**ï¼Œä¸ç®¡tokenåœ¨çª—å£å†…çš„ç»å¯¹ä½ç½®æ˜¯å¤šå°‘ï¼Œåªè¦`(i - j)`æ˜¯ä¸€æ ·çš„ï¼Œå®ƒä»¬çš„attentionå°±ç”¨åŒä¸€ä¸ªbias
- ç¬¬äºŒä¸ªç»´åº¦æ˜¯æ³¨æ„åŠ›å¤´æ•°é‡ï¼Œæ¯ä¸€ä¸ªå¤´å…³æ³¨patchçš„ä¸åŒç»´åº¦åŒºåŸŸ

---

### 3.attnä¸qkv

é‡è¦æ“ä½œï¼š`qkv = self.qkv(x).reshape(â€¦â€¦).permute(â€¦â€¦)`

- `self.qkv(x)`ï¼šä½¿ç”¨çº¿æ€§æ˜ å°„ï¼Œshapeä¸º**[B_, N, 3 Ã— C]**ï¼Œç­‰å¾…æ‹†åˆ†
- `.reshape(B_, N, 3, self.num_heads, C // self.num_heads)`ï¼šä¸ºæ¯ä¸ª head éƒ½ç”Ÿæˆäº†ä¸€ä¸ª`head_dim`çš„ Q/K/Vå‘é‡ï¼Œä½“ç°äº†æ¯ä¸€ä¸ªå¤´å…³æ³¨patchçš„ä¸åŒç»´åº¦åŒºåŸŸ
- `.permute(2, 0, 3, 1, 4)`è°ƒæ¢ç»´åº¦ï¼Œå˜ä¸º**[3, B_, num_heads, N, head_dim]**
``è¿™æ ·å°±æœ‰q, k, v= qkv[0], qkv[1], qkv[2]

`attn = (q @ k.transpose(-2, -1))`ï¼Œ@æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œæ­¤å¤„attnç›¸å½“äº$QK^T$ï¼Œç»´åº¦åœ¨æœ€åè¾“å‡ºæ—¶ä¹Ÿæ˜¯`[B_, num_heads, Wh*Ww, Wh*Ww]`ï¼Œè¡¨ç¤ºæ¯ä¸ªtokenå¯¹å…¶ä»–tokençš„æ³¨æ„åŠ›åˆ†å¸ƒ

---

### 4.mask

ä¼ å…¥çš„maskå½¢çŠ¶ä¸º`[æ€»çª—å£æ•°, å•çª—å£patchæ•°*,* å•çª—å£patchæ•°]`ï¼Œè°ƒæ•´attnçš„åŒæ—¶ï¼Œå°†`mask`å¹¿æ’­åˆ°`[1, nW, 1, Wh*Ww, Wh*Ww]`ï¼Œ-100çš„æ•°å€¼è¶³ä»¥è®©softmaxåçš„æ³¨æ„åŠ›æƒé‡å½’0

```python
if mask is not None:
    nW = mask.shape[0]  # è·å–çª—å£æ•°é‡
    
    # å°†æ³¨æ„åŠ›åˆ†æ•°å˜å½¢ä¸º [B, nW, num_heads, N, N] ä»¥ä¾¿ä¸ mask å¯¹é½
    attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
    
    # æ¢å¤å½¢çŠ¶ [B*nW, num_heads, N, N]
    attn = attn.view(-1, self.num_heads, N, N)
    
    # Softmax å¤„ç†ï¼ˆ-100 çš„ä½ç½®æƒé‡è¶‹è¿‘äº 0ï¼‰
    attn = self.softmax(attn)
```

---

### 5.å¯è®­ç»ƒå‚æ•°

æ€»ç»“WindowAttentionçš„å¯è®­ç»ƒå‚æ•°ç»„æˆï¼š

| æ¨¡å— | å‚æ•° | å¤§å° | å«ä¹‰ |
| --- | --- | --- | --- |
| QKV æ˜ å°„ | `self.qkv` | `[C, 3C]` | æŠŠæ¯ä¸ª token æ˜ å°„ä¸º Q/K/V |
| è¾“å‡ºæ˜ å°„ | `self.proj` | `[C, C]` | æŠŠå¤šå¤´è¾“å‡ºèåˆå› C ç»´ |
| ç›¸å¯¹ä½ç½®åç½® | `self.relative_position_bias_table` | `[(2Wh-1)(2Ww-1), nH]` | ç»™æ¯ä¸ª token å¯¹åŠ åç½®ï¼Œæé«˜ç©ºé—´å»ºæ¨¡èƒ½åŠ› |