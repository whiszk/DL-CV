# Swin Transformer

<aside>
ğŸ‘“

@whiszk

04/11/2025

</aside>

> åŸå§‹è®ºæ–‡ï¼š
[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030)
å‚è€ƒè§†é¢‘ï¼š
[Swin Transformer æ²¹ç®¡coffee break](https://www.youtube.com/watch?v=SndHALawoag&list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6&index=5)
å‚è€ƒåšå®¢ï¼š
[è¯¦è§£Swin Transformer](https://blog.csdn.net/qq_39478403/article/details/120042232)
[å›¾è§£Swin Transformerï¼ˆç»“åˆä»£ç ï¼‰](https://zhuanlan.zhihu.com/p/367111046)
> 

---

## ä¸€ã€æ¦‚å¿µè§£é‡Š

### 1.SwinTä¸æ ‡å‡†T

**Swin Transformer**ï¼ˆShifted Window Transformerï¼‰æ˜¯ä¸€ç§**è§†è§‰é¢†åŸŸ**ä¸“ç”¨çš„Transformeræ¨¡å‹å˜ä½“ï¼Œå®ƒä¿ç•™äº†åŸå§‹Transformerçš„æ ¸å¿ƒæœºåˆ¶ï¼ˆSelf-Attention + FFN + æ®‹å·® + LayerNormï¼‰ï¼Œä½†å¯¹å…¶ç»“æ„è¿›è¡Œäº†é€‚é…æ”¹è¿›ï¼Œä½¿å…¶æˆä¸ºé€‚åˆå›¾åƒä»»åŠ¡çš„backboneï¼Œå¯ä»¥æ›¿ä»£ResNetã€ViTç­‰ç”¨äºåˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ç­‰ä»»åŠ¡ã€‚

å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šåœ¨è§†è§‰ä»»åŠ¡ä¸­ï¼Œå¼•å…¥ä¸€ç§**æ»‘åŠ¨çª—å£**(Shifted Windows) + **å±‚æ¬¡ç»“æ„**(Hierarchical Structure)çš„Transformeræ¶æ„ï¼Œä½¿å…¶èƒ½å¤Ÿï¼š

- é«˜æ•ˆæ•æ‰**å±€éƒ¨ä¸Šä¸‹æ–‡**ä¿¡æ¯ï¼ˆå°çª—å£æ³¨æ„åŠ›ï¼‰
- åˆèƒ½é€æ­¥å»ºæ„**å…¨å±€è¯­ä¹‰**ï¼ˆå¤šå±‚çº§ç»“æ„ + çª—å£æ»‘åŠ¨ï¼‰

| ç‰¹ç‚¹ | åŸå§‹ Transformerï¼ˆNLPï¼‰ | Swin Transformerï¼ˆCVï¼‰ |
| --- | --- | --- |
| ä»»åŠ¡ç±»å‹ | æ–‡æœ¬åºåˆ—å»ºæ¨¡ï¼ˆå¦‚ç¿»è¯‘ï¼‰ | å›¾åƒå»ºæ¨¡ï¼ˆå¦‚åˆ†ç±»/æ£€æµ‹ï¼‰ |
| è¾“å…¥å½¢å¼ | Tokenï¼ˆè¯æˆ–å­—ç¬¦ï¼‰åºåˆ— | Patchï¼ˆå›¾åƒå—ï¼‰åºåˆ— |
| Attention èŒƒå›´ | å…¨å±€ï¼ˆGlobal Attentionï¼‰ | **å±€éƒ¨çª—å£**ï¼ˆWindow-based Attentionï¼‰ |
| ç‰¹å¾ç»“æ„ | ä¸å˜ | å±‚çº§ç»“æ„ï¼ˆHierarchicalï¼‰ |
| ä½ç½®ç¼–ç  | æ˜¾å¼åŠ ä¸Š | ä½ç½®éšå«åœ¨çª—å£åˆ’åˆ†ä¸­ |
| ä¸‹é‡‡æ · | æ—  | æœ‰ï¼ˆPatch Merging å®ç°é‡‘å­—å¡”ç»“æ„ï¼‰ |

---

### 2.SwinTç›¸æ¯”æ ‡å‡†Tçš„æ”¹åŠ¨

çª—å£æ³¨æ„åŠ›ï¼ˆWindow-based Attentionï¼‰

- åœ¨åŸå§‹Transformerä¸­ï¼Œæ¯ä¸ªTokenéƒ½å¯ä»¥ä¸å…¶ä»–æ‰€æœ‰TokenåšAttentionï¼Œä»£ä»·æ˜¯ $O(n^2)$ï¼Œåœ¨å›¾åƒä¸­æå…¶æ˜‚è´µï¼ˆæ¯”å¦‚ä¸€å¼ `224Ã—224`çš„å›¾åƒæœ‰ä¸Šåƒä¸ªpatchï¼‰ã€‚
- Swin å°†Attentioné™åˆ¶åœ¨**å°çª—å£ä¸­å±€éƒ¨è¿›è¡Œ**ï¼ˆæ¯”å¦‚`7Ã—7`ï¼‰ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚

Shifted Windowï¼ˆ**æ»‘åŠ¨çª—å£**ï¼‰æœºåˆ¶

- ç”±äºåªåœ¨å±€éƒ¨åšAttentionï¼Œä¼šä¸¢å¤±å…¨å±€è”ç³»ã€‚
- Swinå¼•å…¥â€œ**æ»‘åŠ¨çª—å£æœºåˆ¶**â€æ¥è®©ä¸åŒçª—å£ä¹‹é—´çš„ä¿¡æ¯æµåŠ¨ï¼Œå¤šæ¬¡ç§»ä½è®©ç›¸éš”å¾ˆè¿œçš„patché—´æ¥äº¤æµï¼Œä»è€Œæ‰“ç ´ç”Ÿç¡¬çš„çª—å£åˆ’åˆ†ã€‚

> ç‚¹å‡»[è¿™é‡Œ](Swin%20Transformer%201d23f1da2cf580378293d425f1832e01/Swin%20Transformer%20Block%201d93f1da2cf5802191aac4a5f286caad.md)ï¼ŒæŸ¥çœ‹å…³äºæ»‘åŠ¨çª—å£çš„éš¾ç‚¹æé†’
> 

é‡‘å­—å¡”ç»“æ„ï¼ˆ**å±‚çº§åŒ–**ï¼‰

- åŸå§‹Transformeræ²¡æœ‰ä¸‹é‡‡æ ·ï¼Œä¸åƒCNNé‚£æ ·æœ‰é‡‘å­—å¡”ç»“æ„ã€‚
- Swinåœ¨æ¯ä¸ªé˜¶æ®µé€šè¿‡â€œPatch Mergingâ€å¯¹å›¾åƒå—è¿›è¡Œä¸‹é‡‡æ ·ï¼Œä½¿å¾—æ¨¡å‹èƒ½é€æ­¥æå–æ›´å¤§æ„Ÿå—é‡çš„ç‰¹å¾ï¼ˆç±»ä¼¼ResNetã€FPNçš„åšæ³•ï¼‰ã€‚

---

## äºŒã€ç»“æ„ä¸€è§ˆ

æ•´ä¸ªæ¨¡å‹é‡‡å–å±‚æ¬¡åŒ–çš„è®¾è®¡ï¼Œä¸»ä½“ç»“æ„ä¸º4ä¸ªStageï¼Œæ¯ä¸ªstageéƒ½ä¼šç¼©å°è¾“å…¥ç‰¹å¾å›¾çš„åˆ†è¾¨ç‡ï¼ŒåƒCNNä¸€æ ·**é€å±‚æ‰©å¤§æ„Ÿå—é‡**

- é¦–å…ˆå¯¹è¾“å…¥çš„å›¾åƒè¿›è¡Œpatchåˆ†è§£ï¼Œéšåè¿›å…¥å„ä¸ªstage
- stage1ï¼špatchç»è¿‡å±•å¹³ä¸çº¿æ€§æ˜ å°„å¾—åˆ°tokenï¼Œè¿›å…¥ç¬¬ä¸€è½®Swin Transformer Block
- stage2ï¼špatch mergeçš„`2Ã—2`é‚»æ¥ + çº¿æ€§å±‚**å‹ç¼©**ï¼Œä½¿patchçš„æ•°é‡å‡å°‘1/4ï¼Œç»´åº¦ï¼ˆæ‰©å¤§4å€åˆå‹ç¼©ä¸€åŠï¼‰å˜ä¸º2Cï¼Œå†è¿›å…¥block
- åé¢çš„stageåŒç†

![                       swin transformeræ¶æ„ç¤ºæ„å›¾ï¼ˆvisioç»˜åˆ¶ï¼‰](Swin%20Transformer%201d23f1da2cf580378293d425f1832e01/%E7%BB%98%E5%9B%BE1.jpg)

                       swin transformeræ¶æ„ç¤ºæ„å›¾ï¼ˆvisioç»˜åˆ¶ï¼‰

å„é˜¶æ®µ**è¾“å…¥è¾“å‡ºç»´åº¦ï¼š**

| Stage | è¾“å…¥å¤§å° | æ“ä½œ | è¾“å‡ºå¤§å° |
| --- | --- | --- | --- |
| é¢„å¤„ç†é˜¶æ®µ | `[B, H, W, 3]` | Patch Partition + Linear Embedding | `[B, (H/4)*(W/4), C]` |
| Stage 1å‡å»Linear Embedding | `[B, (H/4)*(W/4), C]` | Swin BlockÃ—2 | `[B, (H/4)*(W/4), C]` |
| Stage 2 | `[B, (H/4)*(W/4), C]` | Patch Merging + Swin BlockÃ—2 | `[B, (H/8)*(W/8), 2C]` |
| Stage 3 | `[B, (H/8)*(W/8), 2C]` | Patch Merging + Swin BlockÃ—6 | `[B, (H/16)*(W/16), 4C]` |
| Stage 4 | `[B, (H/16)*(W/16), 4C]` | Patch Merging + Swin BlockÃ—2 | `[B, (H/32)*(W/32), 8C]` |

æ³¨ï¼šè¡¨ä¸­çš„Patch Partitionåˆ’åˆ†çª—å£å¤§å°ä¸º`4Ã—4`ï¼Œè¿›å‡ºæ¯ä¸ªblockæ—¶ï¼Œpatchçš„å½¢çŠ¶éƒ½ä¼šè¢«**reshape**ä¸€æ¬¡ï¼Œä¾¿äºçª—å£æ³¨æ„åŠ›çš„è®¡ç®—

---

## ä¸‰ã€è¾“å…¥é¢„å¤„ç†

ä¸ViTç±»ä¼¼ï¼ŒSwin Transformerçš„è¾“å…¥å›¾åƒä¹Ÿéœ€è¦è¢«åˆ’åˆ†æˆPatchï¼Œä½†å…¶åšæ³•æ›´è´´åˆCNNé£æ ¼ï¼š

- **patch partition**ï¼šå°†å›¾åƒåˆ’åˆ†ä¸ºä¸é‡å çš„Patchï¼Œä¾‹å¦‚ï¼Œå¯¹äº`256Ã—256Ã—3`çš„å›¾åƒï¼Œå¯ä»¥åˆ’åˆ†ä¸º`56Ã—56=3136`ä¸ª`4Ã—4Ã—3`ç»´åº¦çš„patchã€‚
- stage1ä¸­çš„**Linear Embedding**ï¼šæŠŠæ¯ä¸ªPatchå±•å¹³ï¼Œå¹¶çº¿æ€§æ˜ å°„ä¸ºä¸€ä¸ªCç»´å‘é‡ï¼ŒCæœ‰å¤šç§é€‰æ‹©ï¼Œæ ¹æ®æ¨¡å‹å¤§å°è‡ªå®šä¹‰ï¼Œæ¯”å¦‚ï¼Œswin-tinyçš„Cä¸º96ï¼Œswin-largeçš„Cä¸º192ã€‚

ä¾‹å¦‚è¾“å…¥å›¾åƒä¸º`[H, W, 3]`ï¼Œåˆ’åˆ†åå¾—åˆ°`N= (H/4)Ã—(W/4)`ä¸ªCç»´patchï¼Œéšåè¿™äº›patch tokensç»è¿‡Linear Embeddingå¤„ç†ï¼Œå†è¢«é¦ˆå…¥è‹¥å¹²å…·æœ‰æ”¹è¿›è‡ªæ³¨æ„åŠ›çš„Swin Transformer blocks

```python
class PatchEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size) # å•çº¯ç»´åº¦å¤åˆ¶ï¼Œå˜ä¸º(img_size, img_size)
        patch_size = to_2tuple(patch_size) # åŒä¸Š
        
        # å‚ç›´ï¼ˆé«˜ï¼‰å’Œæ°´å¹³ï¼ˆå®½ï¼‰æ–¹å‘ä¸Šçš„patchæ•°
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1] # patchæ€»æ•°

        self.in_chans = in_chans # è¾“å…¥çš„é€šé“æ•°
        self.embed_dim = embed_dim # å¸Œæœ›å¾—åˆ°çš„patchç»´åº¦

				 # å·ç§¯æ“ä½œï¼Œå·ç§¯æ ¸å¤§å°ä¸æ­¥é•¿ç­‰äºpatchå¤§å°ï¼Œè½¬æ¢ä¸¾ä¾‹ï¼š(N, 3, 224, 224)->(N, 96, 56, 56)
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        
        # åˆ›å»ºå½’ä¸€åŒ–æ¨¡å—
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        
        # å°ºå¯¸æ£€æŸ¥
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        
        # å¯¹åº”patch partition + stage1ä¸­çš„Linear Embeddingï¼Œ
        x = self.proj(x).flatten(2).transpose(1, 2)
        
        if self.norm is not None:
            x = self.norm(x)
        return x
        
		# flops() å‡½æ•°çš„ä½œç”¨æ˜¯ï¼šä¼°ç®— PatchEmbed æ¨¡å—ä¸­çš„è®¡ç®—é‡ï¼Œå¯ä»¥ç†è§£ä¸ºå¤æ‚åº¦
    def flops(self):
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops
```

è¦ç‚¹ï¼š

- æ³¨æ„patchçš„åˆ’åˆ†çš„è¿‡ç¨‹ä¸ä»…ä»…åªæ˜¯æ•°æ®ç»´åº¦ä»[é«˜ï¼Œå®½ï¼Œé€šé“æ•°]å˜ä¸º[patchæ•°ï¼Œpatchç»´åº¦]çš„ç®€å•è½¬æ¢ï¼Œè€Œæ˜¯å¯¹åŸå§‹åƒç´ å€¼ä¹Ÿè¿›è¡Œäº†**å·ç§¯å¤„ç†**ï¼Œå…¶ä¸­çš„å·ç§¯æ ¸å‚æ•°æ˜¯å¯å­¦ä¹ çš„
- `self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)`
è¿™ä¸ªå·ç§¯æ“ä½œå¯¹åº”æµç¨‹å›¾çš„**patch partition**ï¼Œå¹¶æå‰å®Œæˆäº†çº¿æ€§æ˜ å°„åˆ°Cç»´å‘é‡çš„è¿‡ç¨‹
- `x = self.proj(x).flatten(2).transpose(1, 2)`ï¼š
    - `.flatten(2)`æŠŠåä¸¤ä¸ªç»´åº¦ `[H', W']` æ‹‰å¹³æˆä¸€ä¸ªç»´åº¦ â†’ patch æ•°é‡
    - `.transpose(1, 2)`äº¤æ¢é€šé“ç»´å’Œpatchæ•°é‡ç»´ï¼šå°† `[B, C, N]` â†’ `[B, N, C]`
    - æœ€ç»ˆ shape æ˜¯ `[B, 3136, 96]`

---

## å››ã€S**win Transformer Block**

ç”±äºå†…å®¹è¾ƒå¤šï¼Œæ”¾åœ¨ä¸‹é¢çš„å­é¡µä¸­

[S**win Transformer Block**](Swin%20Transformer%201d23f1da2cf580378293d425f1832e01/Swin%20Transformer%20Block%201d93f1da2cf5802191aac4a5f286caad.md)

---

## äº”ã€Patch Merging

ä½œç”¨ç›¸å½“äºCNNé‡Œçš„poolingï¼ŒSwimçš„ä¸‹é‡‡æ ·æ–¹å¼æ˜¯ï¼š

- å°†ç›¸é‚»çš„`2Ã—2`patchæ‹¼æ¥å¾—åˆ°ç»´åº¦ä¸º`4*C`çš„å¤§token
- å†ç”¨ä¸€ä¸ª Linear å±‚å‹ç¼©ä¸º`2*C`

ç»“æœå°±æ˜¯**å›¾åƒå°ºå¯¸å‡åŠï¼Œé€šé“æ•°ç¿»å€**ï¼š`[B, H, W, C]` â†’ `[B, H/2, W/2, 2C]` 

```python
class PatchMerging(nn.Module):
    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution # è¾“å…¥çš„åˆ†è¾¨ç‡
        self.dim = dim # è¾“å…¥é€šé“æ•°
        
        # ç”¨äºå‹ç¼©çš„çº¿æ€§å±‚ï¼Œ4C->2C
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        
        # å½’ä¸€åŒ–å±‚ï¼Œè¾“å…¥ç»´åº¦æ˜¯ 4C
        self.norm = norm_layer(4 * dim) 

    def forward(self, x):
        H, W = self.input_resolution # é€šè¿‡topæ¨¡å—ä¼ å…¥çš„ï¼Œforwardè¿›æ¥çš„xå¹¶æ²¡æœ‰è¿™ä¸ªä¿¡æ¯
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, C) # reshapeæ“ä½œï¼Œæ–¹ä¾¿è¿›è¡Œç›¸é‚»çš„2Ã—2åˆå¹¶

        x0 = x[:, 0::2, 0::2, :]
        x1 = x[:, 1::2, 0::2, :]
        x2 = x[:, 0::2, 1::2, :]
        x3 = x[:, 1::2, 1::2, :]
        x = torch.cat([x0, x1, x2, x3], -1)  #å˜ä¸º[B H/2 W/2 4*C],åœ¨é€šé“ç»´åº¦æ‹¼æ¥
        x = x.view(B, -1, 4 * C)  #å†æ¬¡reshapeä¸º[B H/2*W/2 4*C]

        x = self.norm(x) # å½’ä¸€åŒ–,é»˜è®¤æ˜¯LayerNorm
        x = self.reduction(x) # å‹ç¼©ï¼Œ4C->2C

        return x

		# å¯é€šè¿‡print(patch_merging)å®ä½“å±•ç¤ºä¿¡æ¯
    def extra_repr(self) -> str:
        return f"input_resolution={self.input_resolution}, dim={self.dim}"

    def flops(self):
        H, W = self.input_resolution
        flops = H * W * self.dim # ä¸viewæœ‰å…³
        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim # å‹ç¼©è¿‡ç¨‹
        return flops
```

å‚æ•°ä¸è¦ç‚¹ï¼š

- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿›å…¥**`PatchMerging`** ä¹‹å‰ï¼Œpatchçš„shapeæ˜¯**`[B, N, C]`**ï¼Œä½†æ˜¯**`PatchMerging` å†…éƒ¨è‡ªå·±åˆæŠŠå®ƒ reshape å› `[B, H, W, C]`**ï¼Œè¿™æ ·å®ƒæ‰èƒ½è¿›è¡Œç©ºé—´ä¸Šçš„2Ã—2é‚»æ¥patchã€‚
    - **`B, L, C = x.shape`**ä»è¿™é‡Œå¯ä»¥çœ‹å‡ºè¾“å…¥çš„å½¢çŠ¶
    - **`x = x.view(B, H, W, C)`**è¿™ä¸ªå°±æ˜¯reshapeæ“ä½œï¼Œ**`view`**æ˜¯pytorchåº“å‡½æ•°
- åˆ‡ç‰‡æ“ä½œï¼Œ**`[start:stop:step]`**ï¼Œæ‰€ä»¥åœ¨æ¯ä¸ª**`2Ã—2`**æ–¹å—ä¸­ï¼Œx0æŠ½å–çš„æ˜¯å·¦ä¸Šè§’patchï¼Œx1æŠ½å–çš„æ˜¯å·¦ä¸‹è§’ï¼Œä»¥æ­¤ç±»æ¨ï¼Œæœ€åç”¨catæŠŠè¿™å››ä¸ªpatchæŒ‰é€šé“ç»´åº¦æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå°±ä»åŸæ¥çš„4ä¸ªCç»´patchï¼Œå˜ä¸º1ä¸ª4Cç»´patch
    - å…³äºæ‹¼æ¥ä¸catå‡½æ•°ç®€å•ç¤ºä¾‹ï¼šåŸå›¾ç‰‡ä¸­ç›¸é‚»å››ä¸ªpacthæ‹¼æ¥
        
        ```
        æ‹¼æ¥å‰ï¼š
        x0 = [[1, 2],    x1 = [[5, 6],    x2 = [[9, 10],   x3 = [[13, 14],
              [3, 4]]          [7, 8]]          [11, 12]]        [15, 16]]
              
        æ‹¼æ¥åï¼š
        x = [
            [[1, 5, 9, 13], [2, 6, 10, 14]],
            [[3, 7, 11, 15], [4, 8, 12, 16]]
        ]
        ```