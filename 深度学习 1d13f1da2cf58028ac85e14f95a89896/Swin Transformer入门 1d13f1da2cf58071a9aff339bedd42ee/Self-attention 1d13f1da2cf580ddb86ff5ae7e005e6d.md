# Self-attention

<aside>
👓

@whiszk 

04/10/2025 

</aside>

---

## 一、概念解释

### 1.注意力与权重

在人类的理解中，处理信息有明显的**侧重点**。举个例子：“我喜欢踢足球，更喜欢打篮球。”，对于人类来说，显然知道这个人更喜欢打篮球。但对于神经网络来说，在不知道”更“这个字的含义前，是没办法知道这个结果的。所以在训练模型的时候，我们会加大“更”字的**权重**，让它在句子中的重要性获得更大的占比。

如：$C(seq)=F(0.1∗d(我)，0.1∗d(喜)，...，0.8∗d(更)，0.2∗d(喜)，...)$

显然此时的问题就变为，如何设定每个单元词的权重？我们把模型进行权重自赋值的过程，称为**self-attention**

需要明确的是：**Self-Attention**是一种**信息融合机制**，它不能单独完成识别任务，但作为模块，它能让模型理解图像的全局结构。

---

### 2.该机制的优点

我们先对这个机制做一个概念上的分析，在NLP中，self—attention的机制能很好捕捉自然语句中的关键词，如前文的例子，这一点很好理解；当我们把它运用到图像识别任务中，这个机制相比于psnr，ssim，lpips等传统指标，也具有很大优势：

- 具备优秀的**全局感知能力**：Self-Attention直接计算每个patch（可以理解为单元图像区域）与**所有其他patch**之间的相似度，而CNN网络需要多次卷积才能把相距甚远的图像块联系在一起。
- 具备全局**语义建模能力**：比如在识别一只狗的图像时，某个 patch 看到的是“耳朵”Self-Attention 可以自动关注“眼睛”“鼻子”等其他语义相关区域，从而组合成“狗脸”的概念。
- **鲁棒性和泛化能力**强：狗可能在图像左边，也可能右边，Transformer的位置编码机制 + 全局注意力让它对空间位置偏移更不敏感，比CNN更擅长处理不规则形状、变形或遮挡的情况

---

### 3.与传统架构的不同

与传统的卷积网络架构和基于距离度量的图像质量指标相比，Self-Attention 提供了一种更加接近人类视觉感知的建模方式。它通过动态建模图像区域间的语义关联，使得模型能够基于“部分”理解“整体”，从而更自然地完成识别与理解任务。

相比之下，卷积核与特征距离更像是在固定框架下进行的统计层面的特征比对，缺乏**主动**的注意力调节能力（就像“人闭着眼睛摸一个东西”，通过点状感觉试图拼出一个整体图案）。

---

## 二、原理详解

### 1.输入与输出的含义

我们先来理解输入与输出的含义，设输入是句子 "The cat sat down"

所以**四个输入向量a1-4**的含义为：

- a1= "The"
- a2= "cat"
- a3= "sat"
- a4= "down"

那么Self-Attention**输出的四个向量b1-4**的含义为：

- b1：仍代表 "The"，但现在它知道了后面 "cat", "sat", "down" 的存在和关系
- b2：是 "cat" 的上下文增强表示，比如它更明确自己是句子的主语，和动作 "sat" 有关联
- b3：可能特别关注 "cat"，因为它要知道是谁在 "sat"
- b4：作为 "down"，它可能从句子中提取出 "sat down" 是一个完整动作的语义

即bi是“全局上下文感知”之后的ai表达，可以看出bi是感知每个ai之后的结果，**是更立体，更有语义的自我**。

---

### 2.QKV结构

接下来我们讨论，每个输入是如何感知其他输入的信息，进而给出更立体的自我呢？

self-attention**把输入映射为多个特征矩阵（Q、K、V）**，这是其核心结构创新之一，

我用一段比喻来深化理解：

> 人类的社会性决定了我们无法表达独立的自我，我们的情绪无时无刻不被与他人的比较所左右，我们的思想无时无刻不被他人的观点所冲击，表达自我的过程就像，我有一双眼睛，即我的**Q[关注偏好]**，我以它观察别人的外在修饰，即别人的**K[他人特征]**，符合我审美的，我更愿意吸纳他的思想，即他的**V[他人思想]**，由此与自我的思想相结合，来表达自我。
> 

这样的结构乍一眼看，好像就是简单地把一个东西拆成三份，但其实还是有不少优势的：

- **可解释性：**相比于传统架构的冰冷参数，注意力权重可直观展示模型关注的位置，我们能更好理解输出的形成过程。
- **动态权重生成：**每个位置的注意力权重由当前输入动态计算，而非固定模式
- **并行计算能力：**自注意力不依赖序列顺序（如RNN的时序计算），而是通过矩阵乘法同时处理所有位置

---

### 3.具体计算

下图是计算$a^1$对所有输入（包括自身）的关注度的过程，如前文所说，每一个输入$a^i$先经过特征矩阵提取qkv，$\alpha_{1,i}$分别是$q^1$与所有输入的$k^i$的计算结果，再用softmax（其他方式也可以）进行归一化，得到关注度。

![Snipaste_2025-04-10_15-51-25.jpg](Self-attention%201d13f1da2cf580ddb86ff5ae7e005e6d/Snipaste_2025-04-10_15-51-25.jpg)

在得到关注度之后，我们把每个输出的$v^i$与关注度进行计算，得到$a^1$的感知性输出$b^1$，

其他$b^i$也是同理，这样就完成了从$a^i$到$b^i$的转换过程。

![Snipaste_2025-04-10_16-03-20.jpg](Self-attention%201d13f1da2cf580ddb86ff5ae7e005e6d/Snipaste_2025-04-10_16-03-20.jpg)

---

我们可以用矩阵运算的方式简化上述流程，假定输入为四个列向量

- 提取特征：输入矩阵I，形状`m×4`，查询权重矩阵$W^q$，形状`d×m`，其他特**征提取矩阵形状也相同**，得到形状为`d×4`的查询矩阵Q，其他特征矩阵得到的结果也是相同
    
    ![Snipaste_2025-04-10_16-19-44.jpg](Self-attention%201d13f1da2cf580ddb86ff5ae7e005e6d/Snipaste_2025-04-10_16-19-44.jpg)
    

- 计算关注度：$K^T$与$Q$做运算，`4×d` 与 `d×4` 做矩阵乘法，得到`4×4`的输出，在经过归一化，得到关注度矩阵：
    
    ![Snipaste_2025-04-10_16-30-18.jpg](Self-attention%201d13f1da2cf580ddb86ff5ae7e005e6d/Snipaste_2025-04-10_16-30-18.jpg)
    

- 得到输出：`d×4`与`4×4`做矩阵乘法，得到`d×4`的输出
    
    ![Snipaste_2025-04-10_16-32-37.jpg](Self-attention%201d13f1da2cf580ddb86ff5ae7e005e6d/Snipaste_2025-04-10_16-32-37.jpg)
    

总结一下，对于输入是列向量的情况，计算输出的公式可以为：

$$
attention = Vsoftmax(\frac{K^TQ}{\sqrt{d}})
$$

---

## 三、multi-head Self-attention

与之前的单头区别不大，multi-head就是把特征通道划分成多块，交给每个头分别处理，每一个头关注token的不同维度区域，比如1-10维给头1，11-20维给头2，**通过训练，每个头会在自己的维度子空间学会关注不同的特征模式**

多头注意力不是简单的单头注意力重复，而是通过不同的投影矩阵让每个头关注输入数据的不同特征子空间或维度区域，对于每一个patch，在不同的head下，它观察别的patch的维度区域是不同的

![Snipaste_2025-04-10_20-54-50.jpg](Self-attention%201d13f1da2cf580ddb86ff5ae7e005e6d/Snipaste_2025-04-10_20-54-50.jpg)

得到多组输出后，通过另一个矩阵进行处理，可以是维度转换，也可以是归一化等等。

![Snipaste_2025-04-10_20-56-27.jpg](Self-attention%201d13f1da2cf580ddb86ff5ae7e005e6d/Snipaste_2025-04-10_20-56-27.jpg)

---

## 四、Positional Encoding

对于上述的self-attention，我们进行矩阵运算时，并没有体现输入间的位置关系，然而无论是自然语言还是图像，内容间的位置关系是十分重要的，比如英文句子开头往往是冠词。

所以我们需要对输入进行位置编码，这里使用的方式是直接为输入$a^i$加上一个位置码$e^i$，这个$e^i$可以是人为设置，也可以是学习得到，方式多种多样。