# 目标检测

日期: 04/19/2025
状态: 完成

> 参考文章：
[目标检测入门（概述）](https://zhuanlan.zhihu.com/p/34142321)
> 

---

## 一、**任务表述**

![v2-68048e7c5272aba98d7fbc96758ad38d_r.jpg](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%201db3f1da2cf5809ba8d8cf2d2f82fbaa/v2-68048e7c5272aba98d7fbc96758ad38d_r.jpg)

关于计算机图像理解，大致可以分为三个层级，从易到难分别是**分类，检测和分割**

- 分类：是将图像结构化为某一类别的信息，用事先确定好的类别(string)或实例ID来描述图片
- 检测：关注特定的物体目标，要求同时获得这一目标在图像中的**类别信息和位置信息**
- 分割：包括语义和实例分割，前者是对前背景分离的拓展，要求分离开具有不同语义的图像部分，而后者是检测任务的拓展，要求描述出目标的轮廓（相比检测框更为精细）

对于一张输入图片，目标检测的任务就是输出：**Bounding Boxes+类别标签**

---

## 二、模型介绍

目标检测使用的模型有很多，这里简单介绍几个经典模型：

### 1.R-CNN

> [原始论文](https://arxiv.org/pdf/1311.2524)
> 

该模型首次将CNN成功应用于目标检测任务，R-CNN将检测抽象为**两个过程**，一是基于图片提出若干可能包含物体的区域（即图片的局部裁剪，被称为**Region Proposal**）；二是在提出的这些区域上运行当时表现最好的分类网络（AlexNet），得到每个区域内物体的类别。两个stage即定位+分类，很自然而然的想法。

核心过程是Region Proposal，对于一张图像，经过Region Proposal后，会输出几千个候选区域等待分类，有很多提取这种区域的算法，比如R-CNN采用的**Selective Search：**

- 使用图像分割算法（如Felzenszwalb算法）生成约1000个初始小区域（超像素，像素群）
- 计算相邻区域的相似度（颜色、纹理、大小、形状等）
- 合并最相似的区域，直到整图合并为一个区域
- 记录合并过程中**所有出现过的区域边界**作为候选框。

不难发现，R-CNN的核心缺陷是Region Proposal与特征提取之间存在严重的重复计算，但模型仍为开创性尝试，为后续的模型提供了优化空间。

---

### 2.Fast R-CNN

> [原始论文](https://arxiv.org/pdf/1504.08083)
> 

Fast R-CNN直接针对R-CNN的**计算冗余问题**，通过**共享卷积计算**，显著提升了速度和精度。R-CNN让每个候选区域独立通过CNN提取特征，其中涉及到将不同大小的区域**强制缩放到固定尺寸**（如227×227），重叠的候选区域在变形后，其重合部分的像素会被CNN重复处理多次，极大浪费计算资源。

为了解决这个问题，Fast R-CNN的做法是：

- 整图特征提取：输入整张图像到CNN（如[VGG16](VGG%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%201cc3f1da2cf580ceb919c5452535fa3b.md)），生成共享的特征图。
- **ROI Pooling**（Region of Interest Pooling）：将候选区域（由Selective Search生成）映射到特征图上，提取固定大小的特征块；比如有一个`14×14×512`的特征图，和一组候选区域坐标（矩阵，四个角），采用具体的映射算法与网格处理后，得到`7×7×512`的特征块，经过展平与全连接完成分类。

效果：

- 计算量从O(N)降至O(1)（N为候选区域数）。
- 速度提升200倍（测试阶段从 50s → 0.2s/图）。

一句话，R-CNN是**先分区，再提取特征，最后分类**；Fast是**先提取特征，再分区，最后分类**。

---

### 3.YOLO

> [原始论文](https://arxiv.org/pdf/1506.02640)
> 

YOLO（You Only Look Once）是**单阶段方法**的开山之作。它将检测任务表述成一个统一的、端到端的回归问题，并且以只处理一次图片同时得到位置和分类而得名，具有**速度快、适合实时检测**的特点。

以YOLOv1举例说明流程：

- 预处理后，将`448×448`的输入图像划分为`7×7`的网格（每个网格`64×64`像素）
- 每个网格负责预测2个边界框（Bounding Box）的坐标和置信度，以及20个类别的概率
- 每个边界框预测5个值：
    - `(x, y)`：框中心相对于当前网格的偏移量（范围 `[0, 1]`）
    - `(w, h)`：框的宽高相对于整个图像的比例（范围 `[0, 1]`）
    - `confidence`：置信度（框内存在物体且定位准确的概率）
- 输出维度：`7×7×30`：
    - 前10维：2个框的 `(x1, y1, w1, h1, conf1, x2, y2, w2, h2, conf2)`
    - 后20维：类别概率分布

这其中的核心，就是backbone（yolo使用CNN）完成的**从输入图像的像素直接映射到每个网格的预测框参数**（坐标、类别、置信度）过程，最后并与人工标注的真实框（Ground Truth）进行比对，通过损失函数优化模型参数，非常直接的操作，框到框。

---

### 4.模型分类

大致有三种分类方法：**检测方式，训练方式，架构设计**，下面为不严谨分类：

| **模型** | **检测方式** | **训练方式** | **架构设计** |
| --- | --- | --- | --- |
| **R-CNN** | Two-Stage | 多阶段 | 基于CNN |
| **Fast R-CNN** | Two-Stage | 端到端 | 基于CNN + ROI Pooling |
| **Mask R-CNN** | Two-Stage | 端到端 | CNN + FPN + ROI Align + 分支 Mask Head |
| **YOLOv1** | One-Stage | 端到端 | 基于CNN |
| **YOLOv8** | One-Stage | 端到端 | CNN + Anchor-Free + Neck |
| **SSD** | One-Stage | 端到端 | 基于CNN + 多尺度预测 |
| **RetinaNet** | One-Stage | 端到端 | 基于CNN + FPN + Focal Loss |
| **DETR** | One-Stage | 端到端 | 基于Transformer |
| **Swin Transformer + FPN** | One-Stage/Two-Stage | 端到端 | 基于Transformer + FPN |
| **Deformable DETR** | One-Stage | 端到端 | 基于Transformer + 可变形注意力 |

---

## 三、训练流程

先**准备数据**：

- 使用工具（如LabelImg）标注图片中的物体边界框和类别，这个过程**必须人工参与**，尤其是对于医学影像或工业检测的场景，需要相关专家深度参与，当然也可以采用半自动化标注，即先让模型进行预标注，再人工修正。
- 在目标检测任务中，训练数据必须带有标记框（标注的边界框和类别），这种训练属于**监督学习**

再**特征提取**，图像通过特征提取网络backbone（如 ResNet、Swin Transformer），得到一个或多个多尺度的特征图。后面就是各种前向传播，损失计算，反向传播等。