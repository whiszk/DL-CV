# LPIPS

<aside>
👓

@whiszk 

04/05/2025 

</aside>

> 参考文献：
> 
> 
> [1801.03924](https://arxiv.org/pdf/1801.03924)
> 

---

## 一、指标原理

**LPIPS**（Learned Perceptual Image Patch Similarity，学习感知图像块相似度）是一种基于深度学习的图像相似度度量标准。与传统的基于像素的图像相似度指标（如 PSNR 和 SSIM）不同，LPIPS尝试模拟人类视觉感知系统对图像质量的判断，重点关注**视觉感知上的相似性**，而非像素级的相似性。

如下图所示：在patch0，1中选取与reference相近的图片，psnr，ssim指标选择的图片往往与人类直觉相背，而有无监督下的lpips都能很好贴近人类选择，可见该指标的强大。

![image.png](LPIPS%201c23f1da2cf580aa9073f8910658d6ba/image.png)

VGG卷积神经网络在原始任务中主要用于图像分类，其本质是通过层层卷积提取图像的语义特征，并最终输出对应类别的预测结果。而在LPIPS指标中，VGG被用作一种**感知特征提取器**，其作用不再是分类，而是提取多层次的语义特征用于图像质量评估。

具体来说，LPIPS将两张待比较的图像同时输入同一个预训练的 VGG 网络，并从多个卷积块的输出中提取特征图。随后，LPIPS对每一层特征图进行**像素级差异比较**（如L2距离），然后通过训练得到的一组线性加权层（1×1卷积）对各层差异进行加权融合，最终得到一个反映图像感知相似性的分数。该分数越小，表示两张图在人类感知上越相似。

> 关于VGG的架构细节，可见链接：
> 
> 
> [VGG卷积神经网络](../VGG%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%201cc3f1da2cf580ceb919c5452535fa3b.md)
> 

---

## 二、代码拆解

### 1.lpips类

```python
路径：/home/whiszk/IQA-PyTorch/pyiqa/archs/lpips_arch.py

class LPIPS(nn.Module):
    def __init__(
        self,
        pretrained=True,  # 是否采用预设的预训练方式
        net='alex',  # 基础网络类型（vgg/alex/squeeze）
        version='0.1',
        lpips=True,  # 是否启用 LPIPS 模式；False情况下，只用平均L2差异计算，不加线性层
        spatial=False,  # 是否返回空间感知 map（如 heatmap），默认只返回单个分数
        pnet_rand=False,  # 主干网络是否使用随机初始化，如果设为False，表示主干网络使用预训练参数（通常在ImageNet上训练过）
        pnet_tune=False,  # 是否训练主干网络的参数，如果设为False，主干网络是冻结的（不参与训练）
        use_dropout=True,
        pretrained_model_path=None,
        eval_mode=True,  # 控制模型是否处于评估模式，为true时，Dropout关闭，
        semantic_weight_layer=-1,  # 控制语义加权策略，增强对关键区域的关注，参数值表示对某一层的索引，默认不使用
        **kwargs,
    ):
        super(LPIPS, self).__init__()

        self.pnet_type = net  # 字符串，初步记录网络类型
        self.pnet_tune = pnet_tune
        self.pnet_rand = pnet_rand
        self.spatial = spatial
        self.lpips = lpips
        self.version = version
        self.scaling_layer = ScalingLayer()  # 用于输入数据标准化的预处理层，固定预设

        self.semantic_weight_layer = semantic_weight_layer
				
				''' 选择主干网络并初始化'''
        if self.pnet_type in ['vgg', 'vgg16']:
            net_type = vgg16  # 这里的net_type被一个类赋值，相当于c++的引用，类实例化在后文，这里只是起了别名
            self.chns = [64, 128, 256, 512, 512]  # 每一层特征图的通道数
        elif self.pnet_type == 'alex':
            net_type = alexnet
            self.chns = [64, 192, 384, 256, 256]
        elif self.pnet_type == 'squeeze':
            net_type = squeezenet
            self.chns = [64, 128, 256, 384, 384, 512, 512]
        self.L = len(self.chns)

        self.net = net_type(pretrained=not self.pnet_rand, requires_grad=self.pnet_tune)

				'''
				构造线性层，线性层的卷积核大小为1×1，初始化需要的参数为backbone网络的各层通道数
				以vgg网络为例，这些线性层的参数总数为(64 + 128 + 256 + 512 + 512) + 5 个 bias = 1472 + 5 = 1477
				'''
        if lpips:
            self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)
            self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)
            self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)
            self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)
            self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)
            self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]
            if self.pnet_type == 'squeeze':
                self.lin5 = NetLinLayer(self.chns[5], use_dropout=use_dropout)
                self.lin6 = NetLinLayer(self.chns[6], use_dropout=use_dropout)
                self.lins += [self.lin5, self.lin6]
            self.lins = nn.ModuleList(self.lins)
				
				'''是否预训练，调用自定义的预训练函数'''
            if pretrained_model_path is not None:
                load_pretrained_network(self, pretrained_model_path, False)
            elif pretrained:
                load_pretrained_network(
                    self, default_model_urls[f'{version}_{net}'], False
                )

        if eval_mode:
            self.eval()
            
```

要点：

- 在选择主干网络并设定每层的通道数时，用了一个临时的net_type变量，接受一个类，相当于c++的引用，类实例化在后文，这里只是起了别名；这样做是为了方便统一网络实例化的方式
- 有时backbone网络是冻结的，不会更新权重，唯一会参与训练的参数是这些线性层（NetLinLayer）里的参数，即1×1卷积核对每个通道的参数权重，以vgg网络为例，这些线性层的参数总数为(64 + 128 + 256 + 512 + 512) + 5 个 bias = 1472 + 5 = 1477
- 调用 `model.eval()` 会把模型切换到“评估模式”，具体影响包括：
    
    
    | 组件 | 训练模式（默认） | 评估模式（`eval()`） |
    | --- | --- | --- |
    | Dropout | 会随机丢弃一部分神经元 | **不丢弃，保持全连接** |
    | BatchNorm | 用当前 batch 的统计量 | **用训练时保存的均值和方差** |
    | 参数更新 | 会被优化器更新 | **不会更新梯度**（通常配合 `torch.no_grad()`） |
    
    函数中`eval_mode`开关的作用时：我可能在别的地方已经训练好了一个LPIPS，要拿过来直接评分，就在 `__init__()` 时进入评估模式
    

```python
def forward(self, in1, in0, retPerLayer=False, normalize=True):
    '''归一化开关，从[0,1] 到 [-1,1]，对所有通道和所有像素位置都执行这一步'''
    if (
        normalize
    ):  
        in0 = 2 * in0 - 1
        in1 = 2 * in1 - 1

    '''图像预处理,并应用backbone网络的前向传播函数'''
    in0_input, in1_input = (
        (self.scaling_layer(in0), self.scaling_layer(in1))
        if self.version == '0.1'
        else (in0, in1)
    )
    outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)
    feats0, feats1, diffs = {}, {}, {}

    for kk in range(self.L):
        feats0[kk], feats1[kk] = (  # 存储处理过的各层特征图
            normalize_tensor(outs0[kk]),
            normalize_tensor(outs1[kk]),
        )
        diffs[kk] = (feats0[kk] - feats1[kk]) ** 2  # 计算两张图片各层特征图的MSE

		'''选择用lpips线性加权来计算评分，或者直接计算各层平均值'''
    if self.lpips:
        if self.spatial:
            res = [
                upsample(self.lins[kk](diffs[kk]), out_HW=in0.shape[2:])
                for kk in range(self.L)
            ]
        elif self.semantic_weight_layer >= 0:  # 使用特定层的语义特征作为权重，对差异分数进行加权平均
            res = []
            semantic_feat = outs0[self.semantic_weight_layer]
            for kk in range(self.L):
                diff_score = self.lins[kk](diffs[kk])
                semantic_weight = torch.nn.functional.interpolate(
                    semantic_feat,
                    size=diff_score.shape[2:],
                    mode='bilinear',
                    align_corners=False,
                )
                avg_score = torch.sum(
                    diff_score * semantic_weight, dim=[1, 2, 3], keepdim=True
                ) / torch.sum(semantic_weight, dim=[1, 2, 3], keepdim=True)
                res.append(avg_score)
        else:  # 直接计算每层差异的空间平均值
            res = [
                spatial_average(self.lins[kk](diffs[kk]), keepdim=True)
                for kk in range(self.L)
            ]
    else:
        if self.spatial:
            res = [
                upsample(diffs[kk].sum(dim=1, keepdim=True), out_HW=in0.shape[2:])
                for kk in range(self.L)
            ]
        else:
            res = [
                spatial_average(diffs[kk].sum(dim=1, keepdim=True), keepdim=True)
                for kk in range(self.L)
            ]

    val = 0
    for i in range(self.L):
        val += res[i]

		'''默认false只返回LPIPS总分val，即所有层的差异分数相加的结果'''
    if retPerLayer:
        return (val, res)
    else:
        return val.squeeze(-1).squeeze(-1)
```

要点：

- 传入的in0，in1是常见的四维tensor格式，先经过缩放再进行归一化
- `outs0`, `outs1`：分别是两张图片在各层的特征图列表
- 得到每层图片之间的差异信息后，我们要进行评分
- 如果启用 LPIPS 模式，有三种情况：① 普通线性加权 + 空间平均 ② 空间输出（不平均）③ 使用某一层语义特征图作为权重
- 如果不启用 LPIPS 模式，直接平均每层的 MSE

### 2.vgg16类（其他用作backbone的CNN也是同理）

```python
class vgg16(torch.nn.Module):
    def __init__(self, requires_grad=False, pretrained=True):
        super(vgg16, self).__init__()
        
        '''
        这里使用了torchvision.models.vgg16 函数来加载 VGG16网络
        通过.features 提取出VGG16网络的前向计算部分（卷积层和池化层）
        再赋值给当前的vgg定制网络
        '''
        vgg_pretrained_features = models.vgg16(weights='IMAGENET1K_V1').features
        self.slice1 = torch.nn.Sequential()
        self.slice2 = torch.nn.Sequential()
        self.slice3 = torch.nn.Sequential()
        self.slice4 = torch.nn.Sequential()
        self.slice5 = torch.nn.Sequential()
        self.N_slices = 5
        for x in range(4):
            self.slice1.add_module(str(x), vgg_pretrained_features[x])
        for x in range(4, 9):
            self.slice2.add_module(str(x), vgg_pretrained_features[x])
        for x in range(9, 16):
            self.slice3.add_module(str(x), vgg_pretrained_features[x])
        for x in range(16, 23):
            self.slice4.add_module(str(x), vgg_pretrained_features[x])
        for x in range(23, 30):
            self.slice5.add_module(str(x), vgg_pretrained_features[x])
        if not requires_grad:  # 如果不需要计算梯度，即要冻结模型参数不进行调整，将规则应用到模型的所有参数
            for param in self.parameters():
                param.requires_grad = False

    def forward(self, X):
    
		    '''前向传播，并保存每个卷积块的输出结果'''
        h = self.slice1(X)
        h_relu1_2 = h
        h = self.slice2(h)
        h_relu2_2 = h
        h = self.slice3(h)
        h_relu3_3 = h
        h = self.slice4(h)
        h_relu4_3 = h
        h = self.slice5(h)
        h_relu5_3 = h
        
        '''返回命名元组'''
        vgg_outputs = namedtuple(
            'VggOutputs', ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3']
        )
        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)

        return out
```

要点：

- 我们自定义这个vgg16类的原因是：官方的vgg module不会保存每一个卷积块的输出结果，而我们计算lpips需要使用这些结果
- 命名规范：h_relu<卷积块序号>_<该块中的卷积层序号>，可参照vgg架构来理解原文命名规则
- `Sequential`是PyTorch中的一种容器，用于将多个神经网络层按顺序组合在一起，它们将会按顺序执行
- vgg_pretrained_features存储的是，vgg网络各层的卷积核，池化方式，激活函数等参数，具体为：
    
    ```python
    Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace=True)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace=True)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace=True)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace=True)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace=True)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace=True)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace=True)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace=True)
      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    ```
    

---

## 三、知识补全

### 1.L1和L2

L1 和 L2 是最基础、最常用的两种**损失函数**，用来衡量两个向量或图像之间的差距，在深度学习中广泛应用。

L1就是差值的**绝对值**之和,即**MAE**（Mean Absolute Error 平均绝对误差）。

$$
L1 = \sum_i|y_i-\hat y_i|

$$

L2就是差值的**平方**之和,即**MSE**（Mean Squared Error 均方误差）。

$$
L2 = \sum_i(y_i-\hat y_i)^2

$$

---

### 2.命名元组

```python
vgg_outputs = namedtuple(
            'VggOutputs', ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3']
        )
out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)
```

关于vgg16中的返回值，以命名元组的方式而不是数值组的方式返回，便于通过字段名访问每个特征图。

`namedtuple` 是 Python 标准库中的一个函数，用来生成一个具名元组类。它的第一个参数是类的名称，通常采用大写字母表示类。例如，`VggOutputs` 就是类的名称，用来表示这个命名元组的类型。
这个命名元组就像弱化的字典，在创建时，顺序确定key值，在初始化时，按顺序将key值与有意义的数值连接在一起，这样我们就可以通过`out.relu1_2`访问第一层输出，而不是`out[0]`，可读性明显提升